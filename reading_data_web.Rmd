---
title: "Reading Data from the Web"
output: github_document
---
```{r}
library(tidyverse)
library(rvest)
library(httr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

## all plots i make will have the viridis color palette
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```


## Scrape a table from a website

I want to first table from this pay (this page http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm)

Read in the html
```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"

drug_use_html = read_html(url)

```

Extract the tables we want. focus on the first table
Still not a tibble but close
```{r}
drug_use_html %>% 
  html_nodes(css = "table")
```

covert table from html to tibble format
focus on the first table
slice(-1) gers rid of first row
```{r}
tbl_marj = 
  drug_use_html %>% 
  html_nodes(css = "table") %>% 
  first() %>% 
  html_table() %>%
  slice(-1) %>% 
  as_tibble()

```


## star Wars Movie Info

I want the data from (here) https://www.imdb.com/list/ls070150896/

```{r}
url = "https://www.imdb.com/list/ls070150896/"

swm_html = read_html(url)
```

Grad elements that I want.

```{r}
titles_vec = 
  swm_html %>% 
  html_nodes(css = ".lister-item-header a") %>% 
  html_text()

gross_revenue_vec =
  swm_html %>% 
  html_nodes(css = ".text-small:nth-child(7) span:nth-child(5)") %>% 
  html_text()

runtime_vec = 
  swm_html %>% 
  html_nodes(css = ".runtime") %>% 
  html_text()

swm_df = 
  tibble(
    title = titles_vec,
    gross_rev = gross_revenue_vec,
    runtime = runtime_vec
  )
```


## Get some water data (NYC Water)

Example in both csv and json file
"parsed" makes it a tibble
```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content("parsed")

## structured differently
nyc_water_json = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.json") %>% 
  content("text") %>% 
  jsonlite::fromJSON() %>% 
  as_tibble()
```


## BRFSS Dataset

Same process, different data
We only have 1000 rows but the website says we have 134k observations
Its a setting with the data
```{r}
brfss_2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv",
      query = list("$limit" = 5000 )) %>% 
   content("parsed")
```

## Some data aren't so nice

Look at Pokemon
```{r}
pokemon_data = 
  GET("https://pokeapi.co/api/v2/pokemon/1") %>% 
  content

pokemon_data$name
pokemon_data$height
```

## Closing thoughts

Make reasonable requests of API server for your data
Have one rmd where you ask servers for the data
Then do analysis in a differ rmd so you don't have to ask for data everytime

